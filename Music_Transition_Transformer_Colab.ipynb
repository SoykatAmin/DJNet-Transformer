{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c377d70",
   "metadata": {},
   "source": [
    "# Music Transition Transformer - Google Colab Notebook\n",
    "\n",
    "This notebook demonstrates the Music Transition Transformer, a transformer-based model for creating seamless transitions between music segments using mel-spectrogram representation.\n",
    "\n",
    "## Features\n",
    "- **Dual Encoder Architecture**: Separate encoders for preceding and following music segments\n",
    "- **Mel-Spectrogram Processing**: Works with frequency-domain audio representation\n",
    "- **Continuous Representation**: Generates smooth spectrograms instead of discrete tokens\n",
    "- **Autoregressive Generation**: Creates transitions step-by-step with proper temporal coherence\n",
    "- **Synthetic Data Support**: Includes synthetic data generation for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2605d5cd",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required dependencies and clone the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04411b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchaudio numpy librosa soundfile matplotlib tqdm scikit-learn\n",
    "\n",
    "# Clone the repository\n",
    "!git clone https://github.com/SoykatAmin/DJNet-Transformer.git\n",
    "\n",
    "# Change to the project directory\n",
    "import os\n",
    "os.chdir('DJNet-Transformer')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d715606f",
   "metadata": {},
   "source": [
    "## Import Libraries and Modules\n",
    "\n",
    "Import all necessary libraries and the custom modules from our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2252c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import our custom modules\n",
    "from music_transformer.model import MusicTransitionTransformer\n",
    "from music_transformer.config import Config\n",
    "from music_transformer.audio_processor import AudioProcessor\n",
    "from music_transformer.dataset import create_synthetic_spectrograms\n",
    "from music_transformer.train import Trainer\n",
    "\n",
    "print(\"All modules imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d90f1a",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "Set up the configuration for our transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510b6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Print configuration details\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  - Mel bins: {config.mel_bins}\")\n",
    "print(f\"  - Sequence length: {config.seq_len}\")\n",
    "print(f\"  - Model dimension: {config.d_model}\")\n",
    "print(f\"  - Number of heads: {config.num_heads}\")\n",
    "print(f\"  - Number of layers: {config.num_layers}\")\n",
    "print(f\"  - Feedforward dimension: {config.d_ff}\")\n",
    "print(f\"  - Dropout: {config.dropout}\")\n",
    "print(f\"  - Learning rate: {config.learning_rate}\")\n",
    "print(f\"  - Batch size: {config.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2f7407",
   "metadata": {},
   "source": [
    "## Initialize the Model\n",
    "\n",
    "Create an instance of the Music Transition Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MusicTransitionTransformer(config).to(device)\n",
    "\n",
    "# Print model information\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model initialized on: {device}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1024 / 1024:.2f} MB (assuming 32-bit floats)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a42b7",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data\n",
    "\n",
    "Create synthetic spectrogram data to test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed046e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic spectrograms only if not using custom dataset\n",
    "if not use_custom_dataset:\n",
    "    print(\"Generating synthetic spectrogram data...\")\n",
    "    spectrograms = create_synthetic_spectrograms(\n",
    "        num_samples=10,\n",
    "        mel_bins=config.mel_bins,\n",
    "        seq_len=config.seq_len * 3  # Total length for preceding + transition + following\n",
    "    )\n",
    "\n",
    "    print(f\"Generated {len(spectrograms)} synthetic spectrograms\")\n",
    "    print(f\"Each spectrogram shape: {spectrograms[0].shape}\")\n",
    "\n",
    "    # Visualize one of the synthetic spectrograms\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.imshow(spectrograms[0], aspect='auto', origin='lower', cmap='viridis')\n",
    "    plt.colorbar(label='Magnitude')\n",
    "    plt.title('Sample Synthetic Mel-Spectrogram')\n",
    "    plt.xlabel('Time Frames')\n",
    "    plt.ylabel('Mel Frequency Bins')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping synthetic data generation - using custom DJNet dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89775b5f",
   "metadata": {},
   "source": [
    "## Dataset Information and Statistics\n",
    "\n",
    "Let's examine the characteristics of our loaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c80d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset statistics\n",
    "print(\"=== DATASET INFORMATION ===\")\n",
    "print(f\"Dataset type: {'DJNet Custom Dataset' if use_custom_dataset else 'Synthetic Dataset'}\")\n",
    "print(f\"Number of samples: {len(spectrograms)}\")\n",
    "print(f\"Spectrogram shape: {spectrograms[0].shape}\")\n",
    "print(f\"Sequence length per segment: {config.seq_len}\")\n",
    "print(f\"Total sequence length: {spectrograms[0].shape[0]} (should be {config.seq_len * 3})\")\n",
    "print(f\"Mel frequency bins: {spectrograms[0].shape[1]}\")\n",
    "\n",
    "# Calculate statistics\n",
    "all_spectrograms = np.array(spectrograms)\n",
    "print(f\"\\n=== SPECTROGRAM STATISTICS ===\")\n",
    "print(f\"Mean magnitude: {all_spectrograms.mean():.4f}\")\n",
    "print(f\"Std magnitude: {all_spectrograms.std():.4f}\")\n",
    "print(f\"Min magnitude: {all_spectrograms.min():.4f}\")\n",
    "print(f\"Max magnitude: {all_spectrograms.max():.4f}\")\n",
    "\n",
    "if use_custom_dataset and 'metadata_df' in locals():\n",
    "    print(f\"\\n=== DJNET DATASET DETAILS ===\")\n",
    "    print(f\"Original dataset size: {len(metadata_df)} transitions\")\n",
    "    print(f\"Successfully processed: {len(spectrograms)} transitions\")\n",
    "    print(f\"Success rate: {len(spectrograms)/len(metadata_df)*100:.1f}%\")\n",
    "    \n",
    "    # Show transition type distribution if available\n",
    "    if len(metadata_df) > 0:\n",
    "        transition_counts = metadata_df['transition_type'].value_counts()\n",
    "        print(f\"\\nTransition types in original dataset:\")\n",
    "        for t_type, count in transition_counts.items():\n",
    "            print(f\"  {t_type}: {count}\")\n",
    "\n",
    "# Visualize spectrogram statistics\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot magnitude distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(all_spectrograms.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Magnitude Distribution')\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Frequency')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot mean spectrogram\n",
    "plt.subplot(2, 3, 2)\n",
    "mean_spec = all_spectrograms.mean(axis=0)\n",
    "plt.imshow(mean_spec.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.colorbar(label='Mean Magnitude')\n",
    "plt.title('Mean Spectrogram Across All Samples')\n",
    "plt.xlabel('Time Frames')\n",
    "plt.ylabel('Mel Frequency Bins')\n",
    "\n",
    "# Plot std spectrogram\n",
    "plt.subplot(2, 3, 3)\n",
    "std_spec = all_spectrograms.std(axis=0)\n",
    "plt.imshow(std_spec.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.colorbar(label='Std Magnitude')\n",
    "plt.title('Standard Deviation Spectrogram')\n",
    "plt.xlabel('Time Frames')\n",
    "plt.ylabel('Mel Frequency Bins')\n",
    "\n",
    "# Plot frequency band energy over time\n",
    "plt.subplot(2, 3, 4)\n",
    "seq_len = config.seq_len\n",
    "time_energy = mean_spec.mean(axis=1)  # Average across frequency bins\n",
    "plt.plot(time_energy[:seq_len], label='Preceding', alpha=0.8)\n",
    "plt.plot(range(seq_len, seq_len*2), time_energy[seq_len:seq_len*2], label='Transition', alpha=0.8)\n",
    "plt.plot(range(seq_len*2, seq_len*3), time_energy[seq_len*2:], label='Following', alpha=0.8)\n",
    "plt.axvline(x=seq_len, color='red', linestyle='--', alpha=0.5, label='Segment Boundaries')\n",
    "plt.axvline(x=seq_len*2, color='red', linestyle='--', alpha=0.5)\n",
    "plt.title('Mean Energy Over Time')\n",
    "plt.xlabel('Time Frames')\n",
    "plt.ylabel('Mean Magnitude')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot frequency band distribution\n",
    "plt.subplot(2, 3, 5)\n",
    "freq_energy = mean_spec.mean(axis=0)  # Average across time\n",
    "plt.plot(freq_energy)\n",
    "plt.title('Mean Energy Across Frequency Bands')\n",
    "plt.xlabel('Mel Frequency Bins')\n",
    "plt.ylabel('Mean Magnitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot sample comparison\n",
    "plt.subplot(2, 3, 6)\n",
    "if len(spectrograms) >= 3:\n",
    "    sample_indices = [0, len(spectrograms)//2, len(spectrograms)-1]\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        sample_energy = spectrograms[idx].mean(axis=1)\n",
    "        plt.plot(sample_energy, alpha=0.7, label=f'Sample {idx}')\n",
    "    plt.title('Energy Comparison Across Samples')\n",
    "    plt.xlabel('Time Frames')\n",
    "    plt.ylabel('Mean Magnitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDataset loaded and ready for model training/testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12465a48",
   "metadata": {},
   "source": [
    "## Load Custom DJNet Dataset (Optional)\n",
    "\n",
    "If you have created a dataset using the DJNet_Colab notebook, you can load and use it here instead of synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44607aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load from Google Drive if you saved the DJNet dataset there\n",
    "use_custom_dataset = True  # Set to False to use synthetic data instead\n",
    "\n",
    "if use_custom_dataset:\n",
    "    try:\n",
    "        # Mount Google Drive if not already mounted\n",
    "        from google.colab import drive\n",
    "        import pandas as pd\n",
    "        import os\n",
    "        \n",
    "        # Try to mount Google Drive\n",
    "        try:\n",
    "            drive.mount('/content/drive')\n",
    "            print(\"Google Drive mounted successfully!\")\n",
    "        except:\n",
    "            print(\"Drive already mounted or not available\")\n",
    "        \n",
    "        # Path to your DJNet dataset (adjust this path based on where you saved it)\n",
    "        djnet_dataset_path = '/content/drive/MyDrive/DJNet_Data/output/djnet_dataset'\n",
    "        \n",
    "        # Alternative paths you might have used\n",
    "        alternative_paths = [\n",
    "            '/content/djnet_dataset',\n",
    "            '/content/drive/MyDrive/djnet_dataset',\n",
    "            './djnet_dataset'\n",
    "        ]\n",
    "        \n",
    "        dataset_found = False\n",
    "        \n",
    "        # Check if dataset exists\n",
    "        if os.path.exists(djnet_dataset_path):\n",
    "            dataset_found = True\n",
    "            print(f\"Found DJNet dataset at: {djnet_dataset_path}\")\n",
    "        else:\n",
    "            # Try alternative paths\n",
    "            for alt_path in alternative_paths:\n",
    "                if os.path.exists(alt_path):\n",
    "                    djnet_dataset_path = alt_path\n",
    "                    dataset_found = True\n",
    "                    print(f\"Found DJNet dataset at: {djnet_dataset_path}\")\n",
    "                    break\n",
    "        \n",
    "        if not dataset_found:\n",
    "            print(\"DJNet dataset not found. Available options:\")\n",
    "            print(\"1. Run the DJNet_Colab notebook first to generate the dataset\")\n",
    "            print(\"2. Upload your dataset to Google Drive\")\n",
    "            print(\"3. Set use_custom_dataset = False to use synthetic data\")\n",
    "            use_custom_dataset = False\n",
    "        else:\n",
    "            # Load metadata\n",
    "            metadata_path = os.path.join(djnet_dataset_path, 'metadata.csv')\n",
    "            if os.path.exists(metadata_path):\n",
    "                metadata_df = pd.read_csv(metadata_path)\n",
    "                print(f\"Loaded metadata for {len(metadata_df)} transitions\")\n",
    "                print(f\"Transition types: {metadata_df['transition_type'].value_counts().to_dict()}\")\n",
    "                \n",
    "                # Display first few entries\n",
    "                print(\"\\nFirst few dataset entries:\")\n",
    "                print(metadata_df[['transition_type', 'source_a_track', 'source_b_track']].head())\n",
    "                \n",
    "            else:\n",
    "                print(\"Metadata file not found in dataset directory\")\n",
    "                use_custom_dataset = False\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading custom dataset: {e}\")\n",
    "        print(\"Falling back to synthetic data...\")\n",
    "        use_custom_dataset = False\n",
    "\n",
    "if not use_custom_dataset:\n",
    "    print(\"Using synthetic data for demonstration...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_djnet_spectrograms(djnet_dataset_path, metadata_df, max_samples=50):\n",
    "    \"\"\"\n",
    "    Load and process DJNet dataset audio files into mel-spectrograms\n",
    "    \"\"\"\n",
    "    print(\"Processing DJNet audio files into mel-spectrograms...\")\n",
    "    \n",
    "    # Initialize audio processor\n",
    "    audio_processor = AudioProcessor(config)\n",
    "    \n",
    "    spectrograms = []\n",
    "    valid_samples = 0\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "    for idx, row in tqdm(metadata_df.iterrows(), total=min(len(metadata_df), max_samples), desc=\"Processing transitions\"):\n",
    "        if valid_samples >= max_samples:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            transition_dir = os.path.join(djnet_dataset_path, row['path'])\n",
    "            \n",
    "            # Load the three audio segments\n",
    "            source_a_path = os.path.join(transition_dir, 'source_a.wav')\n",
    "            target_path = os.path.join(transition_dir, 'target.wav')\n",
    "            source_b_path = os.path.join(transition_dir, 'source_b.wav')\n",
    "            \n",
    "            # Check if all files exist\n",
    "            if all(os.path.exists(p) for p in [source_a_path, target_path, source_b_path]):\n",
    "                # Load audio files\n",
    "                source_a, _ = librosa.load(source_a_path, sr=config.sample_rate)\n",
    "                target, _ = librosa.load(target_path, sr=config.sample_rate)\n",
    "                source_b, _ = librosa.load(source_b_path, sr=config.sample_rate)\n",
    "                \n",
    "                # Convert to mel-spectrograms\n",
    "                mel_a = audio_processor.audio_to_mel_spectrogram(source_a)\n",
    "                mel_target = audio_processor.audio_to_mel_spectrogram(target)\n",
    "                mel_b = audio_processor.audio_to_mel_spectrogram(source_b)\n",
    "                \n",
    "                # Ensure consistent sequence length\n",
    "                target_len = config.seq_len\n",
    "                \n",
    "                # Trim or pad to target length\n",
    "                def pad_or_trim(mel_spec, target_length):\n",
    "                    if mel_spec.shape[1] > target_length:\n",
    "                        return mel_spec[:, :target_length]\n",
    "                    elif mel_spec.shape[1] < target_length:\n",
    "                        pad_width = target_length - mel_spec.shape[1]\n",
    "                        return np.pad(mel_spec, ((0, 0), (0, pad_width)), mode='edge')\n",
    "                    return mel_spec\n",
    "                \n",
    "                mel_a = pad_or_trim(mel_a, target_len)\n",
    "                mel_target = pad_or_trim(mel_target, target_len)\n",
    "                mel_b = pad_or_trim(mel_b, target_len)\n",
    "                \n",
    "                # Combine into one spectrogram (preceding + transition + following)\n",
    "                combined_mel = np.concatenate([mel_a, mel_target, mel_b], axis=1)\n",
    "                spectrograms.append(combined_mel.T)  # Transpose to (time, freq)\n",
    "                \n",
    "                valid_samples += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing transition {idx}: {str(e)[:100]}...\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully processed {len(spectrograms)} transitions from DJNet dataset\")\n",
    "    return spectrograms\n",
    "\n",
    "# Load DJNet spectrograms if custom dataset is available\n",
    "if use_custom_dataset and 'metadata_df' in locals():\n",
    "    djnet_spectrograms = load_djnet_spectrograms(djnet_dataset_path, metadata_df, max_samples=20)\n",
    "    \n",
    "    if len(djnet_spectrograms) > 0:\n",
    "        spectrograms = djnet_spectrograms\n",
    "        print(f\"Using {len(spectrograms)} spectrograms from DJNet dataset\")\n",
    "        print(f\"Each spectrogram shape: {spectrograms[0].shape}\")\n",
    "        \n",
    "        # Visualize one of the DJNet spectrograms\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # Plot the full combined spectrogram\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.imshow(spectrograms[0].T, aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.colorbar(label='Magnitude (dB)')\n",
    "        plt.title('DJNet Transition: Full Sequence (Source A + Transition + Source B)')\n",
    "        plt.xlabel('Time Frames')\n",
    "        plt.ylabel('Mel Frequency Bins')\n",
    "        \n",
    "        # Plot the three segments separately\n",
    "        seq_len = config.seq_len\n",
    "        source_a_mel = spectrograms[0][:seq_len].T\n",
    "        transition_mel = spectrograms[0][seq_len:seq_len*2].T\n",
    "        source_b_mel = spectrograms[0][seq_len*2:].T\n",
    "        \n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.imshow(source_a_mel, aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.title('Source A (Preceding)')\n",
    "        plt.xlabel('Time Frames')\n",
    "        plt.ylabel('Mel Bins')\n",
    "        \n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.imshow(transition_mel, aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.title('Target Transition')\n",
    "        plt.xlabel('Time Frames')\n",
    "        \n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.imshow(source_b_mel, aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.title('Source B (Following)')\n",
    "        plt.xlabel('Time Frames')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Successfully loaded DJNet dataset!\")\n",
    "    else:\n",
    "        print(\"No valid spectrograms found in DJNet dataset, falling back to synthetic data\")\n",
    "        use_custom_dataset = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f71b1b",
   "metadata": {},
   "source": [
    "## Test Forward Pass\n",
    "\n",
    "Test the model's forward pass with teacher forcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6665e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for forward pass\n",
    "batch_size = 2\n",
    "sample_spectrograms = torch.tensor(spectrograms[:batch_size], dtype=torch.float32).to(device)\n",
    "\n",
    "# Split into preceding, transition, and following segments\n",
    "seq_len = config.seq_len\n",
    "preceding = sample_spectrograms[:, :, :seq_len]  # First third\n",
    "transition = sample_spectrograms[:, :, seq_len:2*seq_len]  # Middle third (target)\n",
    "following = sample_spectrograms[:, :, 2*seq_len:]  # Last third\n",
    "\n",
    "print(f\"Input shapes:\")\n",
    "print(f\"  Preceding: {preceding.shape}\")\n",
    "print(f\"  Transition (target): {transition.shape}\")\n",
    "print(f\"  Following: {following.shape}\")\n",
    "\n",
    "# Forward pass with teacher forcing\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(preceding, following, transition)\n",
    "    print(f\"\\nModel output shape: {output.shape}\")\n",
    "    print(f\"Output statistics:\")\n",
    "    print(f\"  Mean: {output.mean().item():.4f}\")\n",
    "    print(f\"  Std: {output.std().item():.4f}\")\n",
    "    print(f\"  Min: {output.min().item():.4f}\")\n",
    "    print(f\"  Max: {output.max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98711a",
   "metadata": {},
   "source": [
    "## Test Autoregressive Generation\n",
    "\n",
    "Test the model's ability to generate transitions autoregressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808749d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test autoregressive generation\n",
    "print(\"Testing autoregressive generation...\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Use the first sample for generation\n",
    "    test_preceding = preceding[:1]  # Take only first sample\n",
    "    test_following = following[:1]\n",
    "    \n",
    "    generated_transition = model.generate(\n",
    "        test_preceding, \n",
    "        test_following, \n",
    "        max_length=seq_len\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated transition shape: {generated_transition.shape}\")\n",
    "    print(f\"Generation statistics:\")\n",
    "    print(f\"  Mean: {generated_transition.mean().item():.4f}\")\n",
    "    print(f\"  Std: {generated_transition.std().item():.4f}\")\n",
    "    print(f\"  Min: {generated_transition.min().item():.4f}\")\n",
    "    print(f\"  Max: {generated_transition.max().item():.4f}\")\n",
    "\n",
    "# Visualize the generated transition\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot preceding segment\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(test_preceding[0].cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Preceding Segment')\n",
    "plt.ylabel('Mel Frequency Bins')\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot generated transition\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(generated_transition[0].cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Generated Transition')\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot following segment\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(test_following[0].cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Following Segment')\n",
    "plt.xlabel('Time Frames')\n",
    "plt.ylabel('Mel Frequency Bins')\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot combined sequence\n",
    "plt.subplot(2, 2, 4)\n",
    "combined = torch.cat([\n",
    "    test_preceding[0], \n",
    "    generated_transition[0], \n",
    "    test_following[0]\n",
    "], dim=1)\n",
    "plt.imshow(combined.cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Complete Sequence (Preceding + Generated + Following)')\n",
    "plt.xlabel('Time Frames')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1849423f",
   "metadata": {},
   "source": [
    "## Training Example\n",
    "\n",
    "Demonstrate how to train the model with synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeee697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the loaded dataset for training (whether custom DJNet or synthetic)\n",
    "print(\"Preparing training data...\")\n",
    "\n",
    "if use_custom_dataset:\n",
    "    print(f\"Using {len(spectrograms)} samples from DJNet dataset for training\")\n",
    "    # Use the loaded DJNet spectrograms\n",
    "    train_data = torch.tensor(spectrograms, dtype=torch.float32)\n",
    "else:\n",
    "    print(\"Generating additional synthetic training data...\")\n",
    "    # Generate more synthetic data for training\n",
    "    train_spectrograms = create_synthetic_spectrograms(\n",
    "        num_samples=50,\n",
    "        mel_bins=config.mel_bins,\n",
    "        seq_len=config.seq_len * 3\n",
    "    )\n",
    "    train_data = torch.tensor(train_spectrograms, dtype=torch.float32)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(model, config, device)\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "if use_custom_dataset:\n",
    "    print(\"Training on real DJ transitions from your custom dataset!\")\n",
    "else:\n",
    "    print(\"Training on synthetic data for demonstration...\")\n",
    "\n",
    "print(\"Starting training demo...\")\n",
    "\n",
    "# Train for a few epochs as demonstration\n",
    "num_epochs = 5 if use_custom_dataset else 3  # More epochs for real data\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Create batches\n",
    "    for i in range(0, len(train_data), config.batch_size):\n",
    "        batch = train_data[i:i+config.batch_size]\n",
    "        if len(batch) < config.batch_size:\n",
    "            continue\n",
    "            \n",
    "        # Split batch into segments\n",
    "        batch_preceding = batch[:, :, :seq_len]\n",
    "        batch_transition = batch[:, :, seq_len:2*seq_len]\n",
    "        batch_following = batch[:, :, 2*seq_len:]\n",
    "        \n",
    "        # Train step\n",
    "        loss = trainer.train_step(\n",
    "            batch_preceding.to(device),\n",
    "            batch_following.to(device),\n",
    "            batch_transition.to(device)\n",
    "        )\n",
    "        epoch_losses.append(loss)\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs+1), losses, 'b-o', linewidth=2, markersize=8)\n",
    "plt.title(f'Training Loss - {\"DJNet Dataset\" if use_custom_dataset else \"Synthetic Dataset\"}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend analysis\n",
    "if len(losses) > 1:\n",
    "    improvement = losses[0] - losses[-1]\n",
    "    improvement_pct = (improvement / losses[0]) * 100\n",
    "    plt.text(0.05, 0.95, f'Loss Improvement: {improvement_pct:.1f}%', \n",
    "             transform=plt.gca().transAxes, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Plot loss distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(losses, bins=max(3, len(losses)//2), alpha=0.7, edgecolor='black')\n",
    "plt.title('Loss Distribution Across Epochs')\n",
    "plt.xlabel('Loss Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training demo completed!\")\n",
    "if use_custom_dataset:\n",
    "    print(\"The model has been trained on your real DJ transition data!\")\n",
    "    print(\"This should produce more realistic transitions compared to synthetic data.\")\n",
    "else:\n",
    "    print(\"The model has been trained on synthetic data for demonstration purposes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ab7ebd",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Evaluate the model's performance after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "test_spectrograms = create_synthetic_spectrograms(\n",
    "    num_samples=10,\n",
    "    mel_bins=config.mel_bins,\n",
    "    seq_len=config.seq_len * 3\n",
    ")\n",
    "test_data = torch.tensor(test_spectrograms, dtype=torch.float32).to(device)\n",
    "\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_data)):\n",
    "        sample = test_data[i:i+1]\n",
    "        \n",
    "        # Split into segments\n",
    "        test_preceding = sample[:, :, :seq_len]\n",
    "        test_transition = sample[:, :, seq_len:2*seq_len]\n",
    "        test_following = sample[:, :, 2*seq_len:]\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(test_preceding, test_following, test_transition)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = nn.MSELoss()(output, test_transition)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "avg_test_loss = np.mean(test_losses)\n",
    "print(f\"Average test loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test loss std: {np.std(test_losses):.4f}\")\n",
    "\n",
    "# Compare original vs generated transition\n",
    "with torch.no_grad():\n",
    "    sample_idx = 0\n",
    "    test_sample = test_data[sample_idx:sample_idx+1]\n",
    "    \n",
    "    original_preceding = test_sample[:, :, :seq_len]\n",
    "    original_transition = test_sample[:, :, seq_len:2*seq_len]\n",
    "    original_following = test_sample[:, :, 2*seq_len:]\n",
    "    \n",
    "    # Generate new transition\n",
    "    generated_transition = model.generate(\n",
    "        original_preceding,\n",
    "        original_following,\n",
    "        max_length=seq_len\n",
    "    )\n",
    "    \n",
    "    # Calculate similarity metrics\n",
    "    mse = nn.MSELoss()(generated_transition, original_transition).item()\n",
    "    mae = nn.L1Loss()(generated_transition, original_transition).item()\n",
    "    \n",
    "    print(f\"\\nComparison with original transition:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(original_preceding[0].cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Original Preceding')\n",
    "plt.ylabel('Mel Bins')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.imshow(original_transition[0].cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Original Transition')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.imshow(original_following[0].cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Original Following')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.imshow(original_preceding[0].cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Same Preceding')\n",
    "plt.xlabel('Time Frames')\n",
    "plt.ylabel('Mel Bins')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.imshow(generated_transition[0].cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Generated Transition')\n",
    "plt.xlabel('Time Frames')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.imshow(original_following[0].cpu().numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Same Following')\n",
    "plt.xlabel('Time Frames')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a661bee",
   "metadata": {},
   "source": [
    "## Audio Processing Example\n",
    "\n",
    "Demonstrate audio processing capabilities (optional - requires audio files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1d571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize audio processor\n",
    "audio_processor = AudioProcessor(config)\n",
    "\n",
    "print(\"Audio Processor Configuration:\")\n",
    "print(f\"  Sample rate: {config.sample_rate} Hz\")\n",
    "print(f\"  Hop length: {config.hop_length}\")\n",
    "print(f\"  N FFT: {config.n_fft}\")\n",
    "print(f\"  Mel bins: {config.mel_bins}\")\n",
    "\n",
    "# Create a synthetic audio signal for demonstration\n",
    "duration = 3.0  # 3 seconds\n",
    "sample_rate = config.sample_rate\n",
    "t = np.linspace(0, duration, int(sample_rate * duration))\n",
    "\n",
    "# Create a simple synthetic audio signal (combination of sine waves)\n",
    "frequencies = [440, 554, 659, 880]  # A4, C#5, E5, A5 (A major chord)\n",
    "synthetic_audio = np.sum([\n",
    "    np.sin(2 * np.pi * freq * t) * np.exp(-t * 0.5)  # Decaying sine waves\n",
    "    for freq in frequencies\n",
    "], axis=0)\n",
    "\n",
    "# Add some envelope and normalize\n",
    "envelope = np.exp(-t * 0.3)\n",
    "synthetic_audio = synthetic_audio * envelope\n",
    "synthetic_audio = synthetic_audio / np.max(np.abs(synthetic_audio))\n",
    "\n",
    "print(f\"\\nSynthetic audio signal:\")\n",
    "print(f\"  Duration: {duration} seconds\")\n",
    "print(f\"  Sample rate: {sample_rate} Hz\")\n",
    "print(f\"  Shape: {synthetic_audio.shape}\")\n",
    "\n",
    "# Convert to mel-spectrogram\n",
    "mel_spec = audio_processor.audio_to_mel_spectrogram(synthetic_audio)\n",
    "print(f\"\\nMel-spectrogram shape: {mel_spec.shape}\")\n",
    "\n",
    "# Visualize the synthetic audio and its mel-spectrogram\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot waveform\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(t, synthetic_audio)\n",
    "plt.title('Synthetic Audio Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot mel-spectrogram\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(mel_spec, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.colorbar(label='Magnitude (dB)')\n",
    "plt.title('Mel-Spectrogram')\n",
    "plt.xlabel('Time Frames')\n",
    "plt.ylabel('Mel Frequency Bins')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAudio processing example completed!\")\n",
    "print(\"Note: To use real audio files, upload them to Colab and use:\")\n",
    "print(\"  audio, sr = librosa.load('path/to/audio.wav', sr=config.sample_rate)\")\n",
    "print(\"  mel_spec = audio_processor.audio_to_mel_spectrogram(audio)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d855b84",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the key features of the Music Transition Transformer:\n",
    "\n",
    "1. **Model Architecture**: Dual encoder transformer for processing preceding and following music segments\n",
    "2. **Synthetic Data Generation**: Created test data for model validation\n",
    "3. **Forward Pass**: Teacher forcing mode for training\n",
    "4. **Autoregressive Generation**: Step-by-step transition generation\n",
    "5. **Training Loop**: Demonstrated training with synthetic data\n",
    "6. **Model Evaluation**: Performance assessment and visualization\n",
    "7. **Audio Processing**: Conversion between audio and mel-spectrogram representations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To use this model with real music:\n",
    "1. Upload your audio files to Colab\n",
    "2. Use the `AudioProcessor` to convert them to mel-spectrograms\n",
    "3. Create proper datasets with real music segments\n",
    "4. Train the model on your data\n",
    "5. Generate smooth transitions between your music pieces\n",
    "\n",
    "### Key Parameters to Experiment With\n",
    "\n",
    "- `seq_len`: Length of input/output sequences\n",
    "- `d_model`: Model dimension (affects capacity)\n",
    "- `num_heads`: Number of attention heads\n",
    "- `num_layers`: Depth of the transformer\n",
    "- `learning_rate`: Training speed\n",
    "- `mel_bins`: Frequency resolution\n",
    "\n",
    "Happy experimenting with music transitions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a028b06",
   "metadata": {},
   "source": [
    "## Dataset Comparison and Results\n",
    "\n",
    "Understanding the differences between training on custom DJNet data vs synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df365f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATASET COMPARISON ANALYSIS ===\")\n",
    "\n",
    "if use_custom_dataset:\n",
    "    print(\"‚úÖ You successfully used your custom DJNet dataset!\")\n",
    "    print(\"\\nüéµ ADVANTAGES OF CUSTOM DJNET DATA:\")\n",
    "    print(\"‚Ä¢ Real DJ transitions with authentic musical patterns\")\n",
    "    print(\"‚Ä¢ Multiple transition types (linear fade, bass swap, etc.)\")\n",
    "    print(\"‚Ä¢ Tempo and key-matched musical segments\")\n",
    "    print(\"‚Ä¢ Professional-quality audio processing\")\n",
    "    print(\"‚Ä¢ Realistic spectral characteristics\")\n",
    "    \n",
    "    if 'metadata_df' in locals():\n",
    "        print(f\"\\nüìä YOUR DATASET STATISTICS:\")\n",
    "        print(f\"‚Ä¢ Total transitions processed: {len(spectrograms)}\")\n",
    "        print(f\"‚Ä¢ Original dataset size: {len(metadata_df)}\")\n",
    "        \n",
    "        if len(metadata_df) > 0:\n",
    "            transition_types = metadata_df['transition_type'].value_counts()\n",
    "            print(f\"‚Ä¢ Transition variety: {len(transition_types)} different types\")\n",
    "            most_common = transition_types.index[0]\n",
    "            print(f\"‚Ä¢ Most common type: {most_common} ({transition_types[most_common]} samples)\")\n",
    "    \n",
    "    print(f\"\\nüéØ TRAINING RESULTS:\")\n",
    "    print(f\"‚Ä¢ Training epochs: {len(losses)} (extended for real data)\")\n",
    "    print(f\"‚Ä¢ Final loss: {losses[-1]:.4f}\")\n",
    "    if len(losses) > 1:\n",
    "        improvement = ((losses[0] - losses[-1]) / losses[0]) * 100\n",
    "        print(f\"‚Ä¢ Loss improvement: {improvement:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüí° EXPECTED OUTCOMES:\")\n",
    "    print(\"‚Ä¢ Generated transitions should sound more natural\")\n",
    "    print(\"‚Ä¢ Better preservation of musical coherence\")\n",
    "    print(\"‚Ä¢ More realistic frequency transitions\")\n",
    "    print(\"‚Ä¢ Improved temporal flow between segments\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  You used synthetic data for this demonstration\")\n",
    "    print(\"\\nüî¨ SYNTHETIC DATA CHARACTERISTICS:\")\n",
    "    print(\"‚Ä¢ Mathematical patterns without musical structure\")\n",
    "    print(\"‚Ä¢ Consistent for testing and development\")\n",
    "    print(\"‚Ä¢ Fast generation and processing\")\n",
    "    print(\"‚Ä¢ Good for model architecture validation\")\n",
    "    \n",
    "    print(f\"\\nüìä SYNTHETIC DATASET STATISTICS:\")\n",
    "    print(f\"‚Ä¢ Generated samples: {len(spectrograms)}\")\n",
    "    print(f\"‚Ä¢ Training epochs: {len(losses)}\")\n",
    "    print(f\"‚Ä¢ Final loss: {losses[-1]:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüöÄ TO USE YOUR CUSTOM DATA:\")\n",
    "    print(\"1. Run the DJNet_Colab notebook to create your dataset\")\n",
    "    print(\"2. Set use_custom_dataset = True in the second cell\")\n",
    "    print(\"3. Adjust djnet_dataset_path to your dataset location\")\n",
    "    print(\"4. Re-run this notebook to train on real DJ data\")\n",
    "\n",
    "print(f\"\\nüîÑ WORKFLOW COMPARISON:\")\n",
    "print(\"‚îå‚îÄ Synthetic Data Workflow ‚îÄ‚îê    ‚îå‚îÄ Custom DJNet Workflow ‚îÄ‚îê\")\n",
    "print(\"‚îÇ 1. Generate synthetic data ‚îÇ    ‚îÇ 1. Run DJNet_Colab      ‚îÇ\")\n",
    "print(\"‚îÇ 2. Train transformer       ‚îÇ    ‚îÇ 2. Load real transitions ‚îÇ\")\n",
    "print(\"‚îÇ 3. Test model              ‚îÇ    ‚îÇ 3. Process audio files   ‚îÇ\")\n",
    "print(\"‚îÇ 4. Basic validation        ‚îÇ    ‚îÇ 4. Train on real data    ‚îÇ\")\n",
    "print(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ 5. Generate realistic DJ ‚îÇ\")\n",
    "print(\"                                   ‚îÇ    transitions           ‚îÇ\")\n",
    "print(\"                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
    "\n",
    "# Performance visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Dataset type indicator\n",
    "axes[0, 0].text(0.5, 0.7, 'DATASET TYPE', ha='center', va='center', \n",
    "                fontsize=16, weight='bold', transform=axes[0, 0].transAxes)\n",
    "dataset_type = 'DJNet Custom' if use_custom_dataset else 'Synthetic'\n",
    "color = 'green' if use_custom_dataset else 'orange'\n",
    "axes[0, 0].text(0.5, 0.3, dataset_type, ha='center', va='center', \n",
    "                fontsize=20, weight='bold', color=color, transform=axes[0, 0].transAxes)\n",
    "axes[0, 0].set_xlim(0, 1)\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Training progress\n",
    "axes[0, 1].plot(range(1, len(losses)+1), losses, 'o-', color=color, linewidth=2, markersize=8)\n",
    "axes[0, 1].set_title('Training Progress', fontsize=14, weight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Dataset characteristics radar chart (simplified)\n",
    "categories = ['Realism', 'Variety', 'Quality', 'Speed', 'Consistency']\n",
    "if use_custom_dataset:\n",
    "    values = [9, 8, 9, 6, 7]  # Custom data scores\n",
    "else:\n",
    "    values = [4, 5, 6, 10, 9]  # Synthetic data scores\n",
    "\n",
    "angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False)\n",
    "values_plot = values + [values[0]]  # Complete the circle\n",
    "angles_plot = np.concatenate([angles, [angles[0]]])\n",
    "\n",
    "axes[1, 0].plot(angles_plot, values_plot, 'o-', color=color, linewidth=2, markersize=8)\n",
    "axes[1, 0].fill(angles_plot, values_plot, alpha=0.25, color=color)\n",
    "axes[1, 0].set_xticks(angles)\n",
    "axes[1, 0].set_xticklabels(categories)\n",
    "axes[1, 0].set_ylim(0, 10)\n",
    "axes[1, 0].set_title('Dataset Characteristics (1-10 scale)', fontsize=14, weight='bold')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Sample spectrogram comparison\n",
    "sample_spec = spectrograms[0]\n",
    "im = axes[1, 1].imshow(sample_spec.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1, 1].set_title(f'Sample Spectrogram - {dataset_type}', fontsize=14, weight='bold')\n",
    "axes[1, 1].set_xlabel('Time Frames')\n",
    "axes[1, 1].set_ylabel('Mel Frequency Bins')\n",
    "plt.colorbar(im, ax=axes[1, 1], label='Magnitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéâ Analysis complete! Your model is trained and ready to generate DJ transitions.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
